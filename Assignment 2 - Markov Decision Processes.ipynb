{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 2\n",
    "*Markov Decision Processes*  \n",
    "Assignment 2 of 5 submitted  \n",
    "Assignment 1: Dueling Bandits\n",
    "\n",
    "Vasken Dermardiros (ID: 260236302)  \n",
    "Reinforcement Learning COMP767 @ RLLAB McGill Winter 2018 Session  \n",
    "Profs. Doina Precup & Pierre-Luc Bacon  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "This assignment will cover Markov Decision Processes (MDP). Unlike last time, we don't need to review a particular paper but instead make use of Puterman's MDP book [1].\n",
    "\n",
    "We've chosen to go with **Track 1** problem on top of answering the mandatory questions.\n",
    "\n",
    "#### Reference\n",
    "[1] Puterman, Martin L. (1994). *Markov Decision Processes: Discrete Stochastic Dynamic Programming*. John Wiley & Sons, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Mandatory Questions\n",
    "## Bellman Optimality Equations\n",
    "\"\"\"Using a contraction argument, show that there exists a solution to the Bellman optimality\n",
    "equations. That is: show that the Bellman optimality operator is a contraction mapping.\"\"\"\n",
    "\n",
    "\n",
    "Contraction mapping, reproduced from [slide 17](https://www.cs.cmu.edu/~katef/DeepRLControlCourse/lectures/lecture3_mdp_planning.pdf): : an operator $F$ on a normed vector space $\\mathcal{X}$ is a $\\gamma$-contraction, for $0 < \\gamma < 1$, provided $\\forall x, y \\in \\mathcal{X}$  \n",
    "\n",
    "$$\n",
    "\\lVert T(x)-T(y) \\rVert \\leq \\gamma \\lVert x-y \\rVert.\n",
    "$$  \n",
    "\n",
    "$F$ converges to a unique fixed point in $\\mathcal{X}$ at linear convergence rate $\\gamma$.\n",
    "\n",
    "In class, Doina used the infinity-norm:\n",
    "\n",
    "$$\n",
    "\\Vert \\mathbf{x} \\rVert_\\infty := \\underset{i}{\\mathrm{max}} \\lvert x_i \\rvert\n",
    "$$\n",
    "\n",
    "The Bellman expectation equation, eq. 6.1.6, p.144 from Puterman [1], with harmonized notation:\n",
    "\n",
    "$$\n",
    "F^\\pi(v) := v = r^\\pi + \\gamma P^\\pi v\n",
    "$$\n",
    "\n",
    "Where $r$ is the reward, $P$ the transition probability matrix and $\\pi$ the policy. We need to show that the equation above is a contraction mapping. Following the mapping definition, we obtain, for state-value functions $a$ and $b$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lVert F^\\pi(a)-F^\\pi(b) \\rVert_\\infty &= \\lVert r^\\pi+\\gamma P^\\pi a \\rVert_\\infty - \\lVert r^\\pi+\\gamma P^\\pi b \\rVert_\\infty \\\\\n",
    "&= \\lVert \\gamma P^\\pi (a-b) \\rVert_\\infty \\\\\n",
    "&\\leq \\lVert \\gamma P^\\pi \\lVert a-b \\rVert_\\infty \\rVert_\\infty \\\\\n",
    "&\\leq \\gamma \\lVert a-b \\rVert_\\infty \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Policy Iteration\n",
    "\"\"\"Show that the values of two successive policies generated by policy iteration are nondecreasing. Assume a finite MDP and conclude (explain why) that policy iteration must terminate under a finite number of steps. Finally, show that upon termination, policy iteration must have found an optimal policy (ie. one which satisfies the optimality equations).\"\"\"\n",
    "\n",
    "\n",
    "First, let's assume we can evaluate $\\mathcal{v}_\\pi$ given $\\pi$ using *policy evaluation*. Next, by defining:  \n",
    "\n",
    "$$\n",
    "\\pi'(s) := \\underset{a}{\\mathrm{argmin}}~q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "we argue that we can take an action $a$ that will result in a higher value under policy $\\pi$:\n",
    "\n",
    "$$\n",
    "q_\\pi(s,\\pi'(s)) \\geq \\mathcal{v}_\\pi(s)  \\\\\n",
    "\\mathcal{v}_{\\pi'}(s) \\geq \\mathcal{v}_\\pi(s)\n",
    "$$\n",
    "\n",
    "for at least one state $s$. [To make action-value term equal to value term, please consult Sutton book p.61 the equations right after 4.8]. Therefore the value of the policy keeps increasing or doesn't change. Since the MDP is finite with countable states and countable actions, the upper bound on the number of iterations needed is equal to:\n",
    "\n",
    "$$\n",
    "\\lvert states^ {\\lvert actions \\rvert} \\rvert \n",
    "$$\n",
    "\n",
    "which is a finite number. So even by brute force evaluation, we would get to the optimal in that many calculations. Since it's a finite and stationary MDP, if the value $\\mathcal{v}_{\\pi'} = \\mathcal{v}_{\\pi}$, meaning the value has converged, then it must be that $\\mathcal{v}_{\\pi'} = \\mathcal{v}^*$, the optimal value and the policy $\\pi' = \\pi^*$ is optimal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Track 1\n",
    "**Objective**. Implement and compare policy iteration, modified policy iteration and value iteration RL techniques. Apply it to the simple 2-state MDP in the assignment description [reproduced below] and also run it on a -- we chose -- a gridworld MDP. Explain and explore how the convergence rate is affected by the discount factor.\n",
    "\n",
    "![Assignment MDP](img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Implementation\n",
    "Onto Python then! We begin with importing numpy and not use scientific notation because we don't care about tiny numbers here.\n",
    "\n",
    "We will start with the policy iterations and then the value iteration for the MDP and then for the gridworld.\n",
    "\n",
    "*Disclaimer: I was already working on the policy/value iteration problems before the assignment and was following DennyBritz' example files. Since I had already used them, I simply copied them here. There are overlaps between this assignment and his examples. I just want to be 100% transparent is all.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code written in Python 2\n",
    "import numpy as np\n",
    "import pprint\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "# Import the gridworld environment from DennyBritz' repo\n",
    "# [https://github.com/dennybritz/reinforcement-learning]\n",
    "from lib_DennyBritz.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Global variable to count the number of policy evaluation iterations\n",
    "    global count_PE    \n",
    "    \n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                if a in env.P[s].keys(): # is the action possible?\n",
    "                    # For each action, look at the possible next states...\n",
    "                    for prob, next_state, reward, done in env.P[s][a]:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        \n",
    "        # Increment policy improvement counter\n",
    "        count_PE += 1\n",
    "            \n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def modified_policy_eval(policy, env, discount_factor=1.0, theta=0.00001, m=50):\n",
    "    \"\"\"\n",
    "    Partially evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    Policy evaluation is limited to an \"m\" number of iterations or until convergence, whichever occurs first.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Global variable to count the number of policy evaluation iterations\n",
    "    global count_PE\n",
    "    \n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    n = 0 # iteration number\n",
    "    while n <= m: # iteration-based stopping criterion\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                if a in env.P[s].keys(): # is the action possible?\n",
    "                    # For each action, look at the possible next states...\n",
    "                    for prob, next_state, reward, done in env.P[s][a]:\n",
    "                        # Calculate the expected value\n",
    "                        v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)            \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        n += 1 # increment counter\n",
    "        \n",
    "        # Increment policy improvement counter\n",
    "        count_PE += 1\n",
    "        \n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        policy_stable = True\n",
    "        # For each state\n",
    "        for s in range(env.nS):\n",
    "            old_action = np.argmax(policy[s])\n",
    "            action_values = np.zeros(env.nA)\n",
    "            \n",
    "            # Look at the possible next actions\n",
    "            # for a, __ in enumerate(policy[s]):\n",
    "            # for a in range(env.nA):\n",
    "            for a in env.P[s].keys(): # only go through possible actions, rest is \"0\"  \n",
    "                \n",
    "                # For each action, look at the possible next states...     \n",
    "                for prob, next_state, reward, done in env.P[s][a]:           \n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                    \n",
    "            # Only select actions that are possible. Sort actions and return 1st best valid action.\n",
    "            for a_maybe in np.argsort(action_values)[::-1]:\n",
    "                if a_maybe in env.P[s].keys():\n",
    "                    new_action = a_maybe\n",
    "                    break\n",
    "                \n",
    "            # Greedily update the policy\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[new_action]\n",
    "\n",
    "        if policy_stable:\n",
    "            return policy, V\n",
    "        \n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    # Global variable to count the number of policy evaluation iterations\n",
    "    global count_PE\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    # Implement!\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            action_values = np.zeros(env.nA)\n",
    "            # Look at the possible next actions\n",
    "            # for a in range(env.nA):\n",
    "            for a in env.P[s].keys(): # only go through possible actions, rest is \"0\"\n",
    "                # For each action, look at the possible next states...\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            v = np.max(action_values[env.P[s].keys()])\n",
    "            \n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "                \n",
    "        # Increment policy improvement counter\n",
    "        count_PE += 1\n",
    "        \n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:            \n",
    "            break\n",
    "\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    # and output a gready deterministic policy\n",
    "    for s in range(env.nS):\n",
    "        action_values = np.zeros(env.nA)            \n",
    "        # Look at the possible next actions\n",
    "        # for a in range(env.nA):\n",
    "        for a in env.P[s].keys(): # only go through possible actions, rest is \"0\"  \n",
    "            # For each action, look at the possible next states...     \n",
    "            for prob, next_state, reward, done in env.P[s][a]:           \n",
    "                action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        # new_action = np.argmax(action_values)\n",
    "        # Only select actions that are possible. Sort actions and return 1st best valid action.\n",
    "        for a_maybe in np.argsort(action_values)[::-1]:\n",
    "            if a_maybe in env.P[s].keys():\n",
    "                new_action = a_maybe\n",
    "                break\n",
    "        policy[s] = np.eye(env.nA)[new_action]\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "# from IPython.core.debugger import Tracer; Tracer()() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2-State MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's first create the environment\n",
    "class TwoStateMDP():\n",
    "    '''\n",
    "    Transition Probabilities: P(s_0 | s_0, a_0) = 0.5, \n",
    "                              P(s_0 | s_0, a_1) = 0.,\n",
    "                              P(s_1 | s_0, a_0) = 0.5, \n",
    "                              P(s_1 | s_0, a_1) = 1, \n",
    "                              P(s_1 | s_0, a_2) = 0, \n",
    "                              P(s_1 | s_1, a_2) = 1\n",
    "    (State s_0 has access to actions a_0 and a_1, while in state s_1 the agent can only choose a_2. \n",
    "    As Preeti Vyas commented, the probabilities are simply zero for the actions that cannot be \n",
    "    taken.)\n",
    "    Rewards: r(s_0, a_0) = 5, r(s_0, a_1) = 10, r(s_1, a_2) = -1\n",
    "    Discount factor : 0.95\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        nS = 2 # number of states\n",
    "        nA = 3 # number of possible actions\n",
    "        \n",
    "        P = {}\n",
    "        # Since the MDP isn't large, I'll just configure it in manually...\n",
    "        # P[s]: for state \"s\", we have actions\n",
    "        # {a: [(prob_to_land_in_s', next_state, reward, done)]}: for action \"a\"\n",
    "        # if the action isn't possible, we won't account for it in the policy and value iteration processes\n",
    "        P[0] = {0: [(0.5, 1, 5., False),(0.5, 0, 5., False)], 1: [(1., 1, 10., False)]}\n",
    "        P[1] = {2: [(1., 1, -1., False)]}\n",
    "\n",
    "        self.nS, self.nA, self.P = nS, nA, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = TwoStateMDP()\n",
    "discount_factor_MDP = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Policy Evaluations: 464\n",
      "\n",
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ -8.5712 -19.9998]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "global count_PE # policy evaluation count, policy improvement is cheap to computer\n",
    "                # not best practice but this simplifies my life\n",
    "count_PE = 0    # reset counter\n",
    "policy, v = policy_improvement(env, discount_factor=discount_factor_MDP)\n",
    "\n",
    "print(\"Number of Policy Evaluations: %i\"% count_PE)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Policy Evaluations: 114\n",
      "\n",
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ -7.1095 -18.538 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modified Policy Iteration\n",
    "# m = 50 for all n\n",
    "global count_PE # policy evaluation count, policy improvement is cheap to computer\n",
    "count_PE = 0    # reset counter\n",
    "policy, v = policy_improvement(env, policy_eval_fn=modified_policy_eval, discount_factor=discount_factor_MDP)\n",
    "\n",
    "print(\"Number of Policy Evaluations: %i\"% count_PE)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Policy Evaluations: 181\n",
      "\n",
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ -8.5696 -19.9981]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "global count_PE # policy evaluation count, policy improvement is cheap to computer\n",
    "count_PE = 0    # reset counter\n",
    "policy, v = value_iteration(env, discount_factor=discount_factor_MDP)\n",
    "\n",
    "print(\"Number of Policy Evaluations: %i\"% count_PE)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the discount factor affects the convergence rate. Mind you, the modified policy iteration has $m$ fixed. We will sweep through different discount factors and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   26.    26.    15.]\n",
      " [   52.    52.    34.]\n",
      " [   63.    61.    43.]\n",
      " [   83.    62.    58.]\n",
      " [  122.    62.    89.]\n",
      " [  464.   114.   181.]\n",
      " [ 1154.   114.   457.]\n",
      " [ 2306.   114.   918.]]\n"
     ]
    }
   ],
   "source": [
    "gamma = [0.5, 0.75, 0.80, 0.85, 0.90, 0.95, 0.98, 0.99]\n",
    "num_tests = len(gamma)\n",
    "tally_PE = np.zeros((num_tests, 3)) # pi, mod pi, vi\n",
    "\n",
    "for i, df in enumerate(gamma):\n",
    "    count_PE = 0\n",
    "    policy_improvement(env, discount_factor=df)\n",
    "    tally_PE[i,0] = count_PE\n",
    "    \n",
    "    count_PE = 0\n",
    "    policy_improvement(env, policy_eval_fn=modified_policy_eval, discount_factor=df)\n",
    "    tally_PE[i,1] = count_PE\n",
    "    \n",
    "    count_PE = 0\n",
    "    value_iteration(env, discount_factor=df)\n",
    "    tally_PE[i,2] = count_PE\n",
    "    \n",
    "print tally_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbd681918d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSNYpuYiISNYpuYiISNYpuYiISNb9f3O6ofWGwKEAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd6a1ba790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot it instead!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ind = np.arange(num_tests)  # the x locations for the groups\n",
    "width = 0.25       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, tally_PE[:,0], width, color='r')\n",
    "rects2 = ax.bar(ind + width, tally_PE[:,1], width, color='y')\n",
    "rects3 = ax.bar(ind + 2*width, tally_PE[:,2], width, color='g')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Number of Policy Evaluations')\n",
    "ax.set_xlabel('Discount Factor')\n",
    "ax.set_title('Number of Policy Evaluations for Different Discount Factors')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('0.5', '0.75', '0.80', '0.85', '0.90', '0.95', '0.98', '0.99'))\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('Policy Iteration', 'Modified Policy Iteration', 'Value Iteration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the bar chart (or the printout), the higher the discount factor, the more policy evaluations. Why? Because a larger discount factor means our agent cares more about future rewards and so it has to look further, which results in more computation. The case for the modified policy evaluation, the number plateau since we've fixed the number of evaluation to $m$.\n",
    "\n",
    "Now let's see if the values and policy for $\\gamma=0.50$ is the same as when $\\gamma=0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50 Discount Factor\n",
      "Policy Probability Distribution:\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ 9. -2.]\n",
      "\n",
      "0.95 Discount Factor\n",
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ -8.5712 -19.9998]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration comparison\n",
    "# 0.50 Discount Factor\n",
    "policy, v = policy_improvement(env, discount_factor=0.50)\n",
    "print(\"0.50 Discount Factor\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "# 0.95 Discount Factor\n",
    "policy, v = policy_improvement(env, discount_factor=0.95)\n",
    "print(\"0.95 Discount Factor\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope. Not the same. So what is happening? Well, the short-sighted agent ($\\gamma=0.50$) is choosing action 1 over action 0. Action 1 has a higher immediate reward (10) over action 0 (5), so it takes it. However, on the long run, the far sighted agent sees the other path to be more lucrative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gridworld\n",
    "Now, let's apply the policy and value improvement iterations to the gridworld problem. I'm still using \"env\" as the environment variable; make sure to run the cell just below before proceeding, otherwise we'll still be using the 2-state MDP from above and given a default discount factor of 1.0, the process will loop forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[3 3 3 3]\n",
      " [0 3 3 2]\n",
      " [0 3 2 2]\n",
      " [1 1 1 3]]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "policy, v = policy_improvement(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[3 3 3 3]\n",
      " [0 3 3 2]\n",
      " [0 3 2 2]\n",
      " [1 1 1 3]]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modified Policy Iteration\n",
    "policy, v = policy_improvement(env, policy_eval_fn=modified_policy_eval)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[3 3 3 3]\n",
      " [0 3 3 2]\n",
      " [0 3 2 2]\n",
      " [1 1 1 3]]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same test as the MDP here to compare discount factor to number of iterations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  20.   20.    4.]\n",
      " [  32.   32.    4.]\n",
      " [  37.   37.    4.]\n",
      " [  44.   44.    4.]\n",
      " [  57.   57.    4.]\n",
      " [  81.   58.    4.]\n",
      " [ 110.   58.    4.]\n",
      " [ 148.   58.    4.]]\n"
     ]
    }
   ],
   "source": [
    "gamma = [0.5, 0.75, 0.80, 0.85, 0.90, 0.95, 0.98, 1.]\n",
    "num_tests = len(gamma)\n",
    "tally_PE = np.zeros((num_tests, 3)) # pi, mod pi, vi\n",
    "\n",
    "for i, df in enumerate(gamma):\n",
    "    count_PE = 0\n",
    "    policy_improvement(env, discount_factor=df)\n",
    "    tally_PE[i,0] = count_PE\n",
    "    \n",
    "    count_PE = 0\n",
    "    policy_improvement(env, policy_eval_fn=modified_policy_eval, discount_factor=df)\n",
    "    tally_PE[i,1] = count_PE\n",
    "    \n",
    "    count_PE = 0\n",
    "    value_iteration(env, discount_factor=df)\n",
    "    tally_PE[i,2] = count_PE\n",
    "\n",
    "# pi, mod pi, vi    \n",
    "print tally_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbd67edaa50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNXVx/HvEVBWBYQoyBqTqDAgywi4oKDRuIIoqAQV\nBUVMXBI1cUte0Pi6xbjExBBiIkQRUN64axKDoMaVAQEXXFBRWQQEQRCR7bx/3DtDM8z01PRMz/TA\n7/M8/XR1bfd0dXWdrnurb5m7IyIiUl67VHcAIiJSMymBiIhIRpRAREQkI0ogIiKSESUQERHJiBKI\niIhkZKdPIGY2zsxuqKayzczuM7Mvzez1Sl63m9n34vAYM/t1Za4/W7L5eZjZEDP7dzbWnabM/cxs\ntpmtMbNLqqjM3mb2XmkxmFk9M3vCzFab2cNVEVNlM7M2ZrbWzGpVdyw7s5xLIGa2wMyWmVmDlHHn\nmdn0agwrWw4DjgZauXuP4hPN7Bwz2xy/KF/Fg8CJ5S3E3Ue6+28qI+CU2Kab2foYW+HjicosoyLM\nrF1MorULx7n7BHc/popD+SUwzd0bufvvK7oyMxttZhtjMlhjZu+b2R/MrEXhPO7+orvvlyaGgcBe\nwJ7uPqiiMZVX6o+bUqan7vdrzezj+EPrB4XzuPun7t7Q3TdXTdTJxfj/W8Y8JX1/Dq5AmbXjdm2X\n6ToykXMJJKoFXFrdQZRXBr+G2gIL3P3rNPO84u4NgcbAX4GHzKxJpjFWsovil7jwcVJ1B5SD2gJv\nZ7JgavIrZrK7NwKaAgOAvYGZqUmkjBjaAu+7+6ZKjKmyFe73ewA/BL4hvMe8Kiq/KhT//rxSXYFk\n/Lm6e049gAXAVcBKoHEcdx4wPQ63AxyonbLMdOC8OHwO8BJwB7AK+Ag4JI7/DFgGDE1ZdhwwBngW\nWAM8D7RNmb5/nLYSeA84rdiyfwKeBr4GfljC+2kJPB6Xnw+cH8cPB9YDm4G1wHUlLHsO8N+U1w3i\ne8+Pr8+P61wZy2iZMq8D30uJ84aUaf2B2cBXwIfAscAgYGax8i8DHivlcyra5iVMmwecmPK6NrAc\n6BZfPwx8DqwGXgA6FtumN5T0/kt4XycAb8T38RkwOmW+T+O8a+Pj4BK25yHAjBjHDOCQYu/vN3Ff\nWgP8G2gWp9UFHgBWEPaxGcBeJWyH5+Lnuz7G8APCAfHvcXt8AvwK2KWEfXdF6meWss7RwAPFxtUC\n5gC3xdd9gIWlxDAR2ABsjK+Hx/mGxc/tS+BfbPsdcOCnwAfAxwm/F38Enorb7jVg3zjthbi+r2P5\np5e136eMfxKYUtJxIC7zUSzvY2BIynLnx/e2BniHrfvhAfFzXkVIsP1K27+LxxTLHhm3yar4fi2u\nM/V7vSqD788fgIWE/br4flkb+DXhe/sVUEA4xrxcbLueGucfSThGrAAeBVqkrMeBn8Tp8wknFL8n\nHCNXA3OBDmmP1+kmVseDkEB+CPyDrQeS8iaQTcC5hC/WDYSDyR+B3YBj4o7UMGVnXwMcHqffVbij\nEA7Yn8V11Qa6Al8UbtS47Grg0Ljx65bwfl4A7iEcdLoQDhxHpvuilLTTxvIvjbHuARwZY+kW474b\neKHYDr5dAgF6xJiPjjHvQzgY7EY4GByQso43CnfEcn4B/geYkPL6BGBeyuthQKNY5p3A7GIHn6QJ\npA/QKb6PzsBS4OQ0+0nq9mxKOFieFbft4Ph6z5T39yHhoF8vvr45TrsAeAKoT9jHugO7J9lOhOTx\nWHz/7YD32XoQP4ew714cY6pXwvpGUyyBxPHXA6+lbJeFaWLYZh2EHxTzCQe/2oSk9nKxbf5s3Gb1\nSPa9WEHY12oDE4BJJX2GZe33xcYPA5YW/3xjPF8B+8VpLYg/Sgg/jBYBBxEO8N8jnIHVie/5GmBX\nwvdpTco6im+zbWKKZT9JqBloQ/heH5vke53g+3NW3Na1gStj/LvFaVcTfix8n7Dfd0mZ14F2Kes5\nhpAMuhCOP/cAz6UcTxz4J9Akfq4nAK8Tji+7AB2AvdO9j1ytwoJwELrYzJpnsOzH7n6fh/rRyUBr\n4Hp3/9bd/034BZZaB/uUu7/g7t8C1wIHm1lr4ERCFdN97r7J3d8A/o+wUxZ6zN1fcvct7r4+NYi4\njkOBK919vbvPBu4Fzi7He+llZqsIv9gHAwPcfTUwBPibu8+KcV8d425XxvqGx+WejTEvcvd34zom\nA2fG2DsSvqRPplnX781sVcqjsJ3lQaCfmdWPr39M+OULgLv/zd3XxDJHAwea2R7JNsdW7j7d3d+M\n72NuLOOIhIufAHzg7vfHz3Yi8C6QWg13n7u/7+7fAA8RvogQfr3vSTgIbnb3me7+VVkFxirOM4Cr\n4/tfAPyOcMAotNjd744xfZPwvQAsJhxIMjESuMnd53mo1roR6GJmbVPmucndV8aYknwvHnH31+P6\nJrB121VEuve4Bcgzs3ruvsTdC6vszgNudfcZHsx390+AXkBDwo+CDe7+HGFfH1yOeG5291Xu/ikw\njfK/x9Tvz6zCkXGfXBm33a3A7mw9Xp0HXOPuH8T9fra7ryxl/UOAe+M86wk1O0eYWauUeW509y/j\n57oxlrV/jOMdd/883RvI2QTi7m8RPtCrMlh8acrwN3F9xcc1THn9WUq5awm/xFsSfqn0TD1IEj6U\nvUtatgQtgZXuviZl3CeEX/1Jverujd29mbv3cvf/pKz7k2Jxr0iw7taEX9YlGQ/82MyMcFB7KB7k\nS3NJjK3w8esYy3xClcFJMYn0IyQVzKyWmd1sZh+a2VeEM06AZmXEvR0z62lm08xsuZmtJhwIk65n\nm+0XFf9sUr8869i6z9xPqOaZZGaLzexWM6uToMxmhF++qeUWLzPd/pTOPoT9NhNtgbtS9vGVhF/r\npcWV5HtR2rariBLfo4c2xNMJn/8SM3vKzPaPk0vb31sCn7n7lpRx5f1uVvQ9pn5/uhWONLNfmtm7\ncZ/+knCGVbhfp/v+Flf8GPFVXF+Jn2v8cT2GUC2/NF692ShdATmbQKJRhPrL1Ddc2OBcP2Vc6o6b\nidaFA2bWkPArZzFh4z5f7CDZ0N0vTFnW06x3MdC02IfQhnBKWlGLCV/kwrgbEH4Vl7Xuz4B9S5rg\n7q8Szs56E84a7q9AfBMJv+b6A+/EpEJcb39CNeUehLMcCAes4r4m5XM2s+Kf84OEtp/W7r4HYecv\nXE+6zwWKbb8o0Wfj7hvd/Tp370BoRzmRZGeVXxB+5aWWW7zMsuLejpntQjhzerG8y0afARcU28/r\nufvLpcSV5HuRDQMo5T26+7/c/WhC9dW7wF9SYi1pf18MtI7brlDqZ7HNvkf5jjHl/gwLmVlfQtvj\nqYTqsSaENo3C/bq091NSmcWPEY3i+krd39z9zpjM8ghVWJelizenE0g86EwGLkkZt5ywAc6Mv2aH\nUcoBsRyON7PDzGxXQsPpq+7+GeEM6AdmdpaZ1YmPg8zsgITxf0Zo3LrJzOqaWWdCFdIDFYwXwgH6\nXDPrYma7EaodXovVIun8NS53lJntYmb7pPxag1BH/wdgo7unvRSxDJMIdbAXEs8+okbAt4Szpfox\n7tLMATrG91iXUN2VqhHhDG+9mfUgJKdCywnVGt8tZd1PEz7bH8dLIE8nfGHSVdkB4UtuZp1ildRX\nhKSwpYzF8FCl+hDwv2bWKFYRXUaG+0OM+wDCvrA3cHsm6yEk3qtjtSVmtoeZpbu8t0LfC0INQWmf\nyzbid7y9md1NaNu5roR59jKz/vFH1LeEA27h53EvcIWZdbfge3G7v0Y4a/hljL8PIQlPisvNBk4x\ns/oWLjkenvC9Fb6/VvF4Ul6NCO1gXxDOVkcTzkAK3QvcYGb7xvfTxcyaxn1rBdtu14nAcDPrHI8R\nNwEvuvvCkgo2sx7xUZuQQDdQxn6d0wkkup5tNyCEs5JfEDZYR8JBuiIeJJztrCQ0iJ4JEKuejiHU\nWy8mnLLeQmj8TWow4Vf2YuARYFRKNVTG4jp+Tah7XkJIomckWO51QuPnHYTG9OfZ9hfx/YRfH0kO\nan+wba9jn5lSzhLgFcIv9Mkpy/ydcFq9iHBFzKtpYn2f8Pn/h3C1S/GE9hPgejNbQ2gzeyhl2XXA\n/wIvxWqWXsXWvYJw5nA5YT/6JeHKsS8SvO+9gSmE5DGPsA2Tnq1dTPhyfhTfz4PA3xIuW+h0M1tL\n+PweJ8Tf3d0Xl3M9ALj7I4T9elKsVnwLOC7N/BX9XowGxsfP5bRS5jk4vsevCA3OuwMHufubJcy7\nCyERLyZ8h48g/HDB3R8m7AcPEhrJHwWauvsGQsI4jnCwvgc4293fjeu8g3AAXUqo2p2Q8L1BuPLt\nbeBzM0uyP6V6mq37+wLC+1+SMv238T1MjdPGEhrIIRzDHozb9RR3/yfh+/NIXEcbQlVjaQr/KrAq\nlr2EMn6UmHvGZ1uyAzKzeoQrN7q5+wfVHY+I5K6acAYiVetCYIaSh4iUpar+VSo1gJktIDTWnVzN\noYhIDaAqLBERyYiqsEREJCM1ugqrWbNm3q5du+oOQ0SkRpk5c+YX7p5JLx/bqNEJpF27dhQUFFR3\nGCIiNYqZFe+FISOqwhIRkYwogYiISEaUQEREJCM1ug2kJBs3bmThwoWsX7++7JlFKqhu3bq0atWK\nOnWSdMYrsmPZ4RLIwoULadSoEe3atSP0Si6SHe7OihUrWLhwIe3bt6/ucESq3A5XhbV+/Xr23HNP\nJQ/JOjNjzz331Nmu7LR2uAQCKHlIldG+JjuzHTKBiIhI9u34CcSsch8J1KpViy5dupCXl8egQYNY\nt25d2vkbNgx3wly8eDEDBw6s8Fvu06dP0R8sb7wx3f2aym/cuHEsXrz1thPnnXce77zzTqWWISI1\nw46fQKpBvXr1mD17Nm+99Ra77rorY8aMSbRcy5YtmTJlSqXGkkkC2bx5c6nTiieQe++9lw4dOmQU\nm8gOoRJ/fNY0SiBZ1rt3b+bPD7cDv/3228nLyyMvL48777xzu3kXLFhAXl4eEA7iV1xxBXl5eXTu\n3Jm7776b5557jpNP3trT+rPPPsuAAQNKLfuqq67im2++oUuXLgwZEm5E9sADD9CjRw+6dOnCBRdc\nUJQsGjZsyOWXX86BBx7IK6+8wvXXX89BBx1EXl4eI0aMwN2ZMmUKBQUFDBkyhC5duvDNN99sc7Yz\nceJEOnXqRF5eHldeeWVRHA0bNuTaa6/lwAMPpFevXixdurSCW1VEcoK719hH9+7dvbh33nln2xFQ\nuY8EGjRo4O7uGzdu9H79+vk999zjBQUFnpeX52vXrvU1a9Z4hw4dfNasWdvM//HHH3vHjh3d3f2e\ne+7xU0891Tdu3Oju7itWrPAtW7b4fvvt58uWLXN398GDB/vjjz++XflHHHGEz5gxY5t1F26bE088\n0Tds2ODu7hdeeKGPHz8+biZ88uTJRfOuWLGiaPjMM88sKid13amvFy1a5K1bt/Zly5b5xo0bvW/f\nvv7II48Urbtw+V/84hf+m9/8JtF2rCm22+dk51KJx46qAhR4JRyDdQaSBYW/+vPz82nTpg3Dhw/n\nv//9LwMGDKBBgwY0bNiQU045hRdffLHUdfznP//hggsuoHbt8Fedpk2bYmacddZZPPDAA6xatYpX\nXnmF444r9dbV25k6dSozZ87koIMOokuXLkydOpWPPvoICO02p556atG806ZNo2fPnnTq1InnnnuO\nt99+O+26Z8yYQZ8+fWjevDm1a9dmyJAhvPDCCwDsuuuunHjiiQB0796dBQsWJI5ZRHLXDvdHwlxQ\n2AaSDeeeey4nnXQSdevWZdCgQUUJJgl3Z+jQodx0003bTatbty61atUCwn9pfvKTn1BQUEDr1q0Z\nPXp0hf7rUKdOnaLLXWvVqsWmTZsyXpeI5A6dgVSR3r178+ijj7Ju3Tq+/vprHnnkEXr37l3q/Ecf\nfTR//vOfiw62K1euBEJDe8uWLbnhhhs499xzyyy3Tp06bNy4EYCjjjqKKVOmsGzZsqJ1fvLJ9r06\nFyaLZs2asXbt2m0a9hs1asSaNWu2W6ZHjx48//zzfPHFF2zevJmJEydyxBFHlBmfiNRcO/4ZSI7c\nsrdbt26cc8459OjRAwiXv3bt2rXU+c877zzef/99OnfuTJ06dTj//PO56KKLABgyZAjLly/ngAMO\nKLPcESNG0LlzZ7p168aECRO44YYbOOaYY9iyZQt16tThj3/8I23btt1mmcaNG3P++eeTl5fH3nvv\nzUEHHVQ07ZxzzmHkyJHUq1ePV155pWh8ixYtuPnmm+nbty/uzgknnED//v3LtY1EpGap0fdEz8/P\n9+I3lJo3b16iA2tNdtFFF9G1a1eGDx9e3aEIO8c+J2kkvUQ3h461ZjbT3fMrup6sVWGZ2d/MbJmZ\nvVXCtMvNzM2sWXxtZvZ7M5tvZnPNrFu24qrpunfvzty5cznzzDOrOxQR2cllswprHPAH4O+pI82s\nNXAM8GnK6OOA78dHT+BP8VmKmTlzZnWHICICZPEMxN1fAFaWMOkO4JdA6vlcf+Dv8RLlV4HGZtYi\nW7GJiEjFVelVWGbWH1jk7nOKTdoH+Czl9cI4rqR1jDCzAjMrWL58eZYiFRGRslRZAjGz+sA1wP9U\nZD3uPtbd8909v3nz5pUTnIiIlFtVXsa7L9AemBP/VNYKmGVmPYBFQOuUeVvFcSIikqOqLIG4+5vA\ndwpfm9kCIN/dvzCzx4GLzGwSofF8tbsvqYxyp0+v3F4w+/Qp+1I8M2PIkCE88MADAGzatIkWLVrQ\ns2dPnnzyycRltWvXjoKCApo1a8YhhxzCyy+/DMAvfvELnn76aY4//nj23Xdf6tevz9lnn514vQ0b\nNmTt2rXbja9VqxadOnVi06ZNHHDAAYwfP5769euXuZ7FixdzySWXVLgn4T59+nDbbbeRn5/PjTfe\nyDXXXFOh9aUaN24cxxxzDC1btgTC/2wuu+wy9SQsUgFZSyBmNhHoAzQzs4XAKHf/aymzPw0cD8wH\n1gFl/8U6hzVo0IC33nqLb775hnr16vHss8+yzz4lNukkVpg8AMaOHcvKlSuLuh6pLKldsAwZMoQx\nY8Zw2WWXlblctrqhL28C2bx5c6nbZNy4ceTl5RUlkHvvvbfCMYrs7LJ5FdZgd2/h7nXcvVXx5OHu\n7dz9izjs7v5Td9/X3Tu5e0HJa605jj/+eJ566ikgdHM+ePDgomkrV67k5JNPpnPnzvTq1Yu5c+cC\nsGLFCo455hg6duzIeeedR+qfPAtvOtWvXz/Wrl1L9+7dmTx5MqNHj+a2224D4MMPP+TYY4+le/fu\n9O7dm3fffReAjz/+mIMPPphOnTrxq1/9KlH86oZe3dCLlEV9YWXJGWecwaRJk1i/fj1z586lZ8+t\nf2sZNWoUXbt2Ze7cudx4441F1U/XXXcdhx12GG+//TYDBgzg008/3W69jz/+eNGZwumnn77NtBEj\nRnD33Xczc+ZMbrvtNn7yk58AcOmll3LhhRfy5ptv0qJF2VdHb9q0iWeeeYZOnToxc+ZM7rvvPl57\n7TVeffVV/vKXv/DGG2+UuuzYsWNZsGABs2fPZu7cuQwZMoS+ffvy7rvvUnjV3H333cewYcNKXcfN\nN99c9B4nTJjAvHnzmDx5Mi+99BKzZ8+mVq1aTJgwAYCvv/6anj17MmfOHA477DAuuugiZsyYUXQG\n+OSTTzJw4EDy8/OZMGECs2fPpl69ekVlLV68mCuvvJLnnnuO2bNnM2PGDB599NGidffq1Ys5c+Zw\n+OGH85e//KXMbSeyM1ECyZLOnTuzYMECJk6cyPHHH7/NtP/+97+cddZZABx55JGsWLGCr776ihde\neKHoH+YnnHACTZo0SVze2rVrefnllxk0aFDRr/QlS0Iz0ksvvVR0BlRYbknUDb26oRcpjx2/M8Vq\n1K9fP6644gqmT5/OihUrslrWli1baNy4candyFuC/nrUDf221A29SHo6A8miYcOGMWrUKDp16rTN\n+N69exdVwUyfPp1mzZqx++67c/jhh/Pggw8C8Mwzz/Dll18mLmv33Xenffv2PPzww0A46M6ZE/6v\neeihhzJp0iSAonKTUjf0IlKaHf4MJMllt9nSqlUrLrnkku3Gjx49mmHDhtG5c2fq16/P+PHjgdA2\nMnjwYDp27MghhxxCmzZtylXehAkTuPDCC7nhhhvYuHEjZ5xxBgceeCB33XUXP/7xj7nlllvK3cW6\nuqEXkdKoO3epMjtqN/Ta53ZyO3F37jv8GYjkhu7du9OgQQN+97vfVXcoIlJJlECkSqgbepEdjxrR\nRUQkI0ogIiKSkTITiJkdamYN4vCZZna7mbUtazkREdmxJTkD+ROwzswOBC4HPqTYbWpFRGTnk6QR\nfZO7e7yb4B/c/a9mVmOuw7TrKrc7dx+V/lK8vn37ctVVV/GjH/2oaNydd97Je++9x5/+9KdSlyut\ni/XyGj16NA0bNuSKK67Yrgvzipo+fTq77rorhxxyCABjxowpd1fyIrLjSHIGssbMrgbOBJ4ys12A\nOtkNq+YaPHhw0b++C02aNGmb3niryrhx41i8eHG5lknXXcf06dO36VZ+5MiRSh4iO7EkCeR04Ftg\nuLt/Trhb4G+zGlUNNnDgQJ566ik2bNgAhK7OFy9eTO/evVm7di1HHXUU3bp1o1OnTjz22GPbLT99\n+vSiDvwg/Plu3LhxQLgU9ogjjqB79+786Ec/KuossSQldWFe2vJ9+vThZz/7Gfn5+dx111088cQT\n9OzZk65du/LDH/6QpUuXsmDBAsaMGcMdd9xBly5dePHFF7fpSn727Nn06tWLzp07M2DAgKJuWPr0\n6cOVV15Jjx49+MEPfpC2I0YRqVnKTCDu/rm73+7uL8bXn7q72kBK0bRpU3r06MEzzzwDhLOP0047\nDTOjbt26PPLII8yaNYtp06Zx+eWXk7QngI0bN3LxxRczZcoUZs6cybBhw7j22mtLnb94F+a1a9dO\nu/yGDRsoKCjg8ssv57DDDuPVV1/ljTfe4IwzzuDWW2+lXbt2jBw5kp///OfMnj17u/6wzj77bG65\n5Rbmzp1Lp06duO6664qmbdq0iddff50777xzm/EiUrOV2QZiZqcAtxBuR2vx4e6+e5Zjq7EKq7H6\n9+/PpEmT+Otfw7203J1rrrmGF154gV122YVFixaxdOlS9t577zLX+d577/HWW29x9NFHA+HGTUnu\n7ZF0+dR7iyxcuJDTTz+dJUuWsGHDBtq3b5923atXr2bVqlVFnRAOHTqUQYMGFU0/5ZRTAHWJLrKj\nSdKIfitwkrvPy3YwO4r+/fvz85//nFmzZrFu3Tq6d+8OhM4Oly9fzsyZM6lTpw7t2rXbrrvx2rVr\ns2XLlqLXhdPdnY4dO27TEWB5lLV8gwYNioYvvvhiLrvsMvr168f06dMZPXp0RmUW2m233QB1iS6y\no0nSBrJUyaN8GjZsSN++fRk2bNg2jeerV6/mO9/5DnXq1GHatGkldknetm1b3nnnHb799ltWrVrF\n1KlTAdhvv/1Yvnx5UQLYuHFjmTdLSu3CvDzLr169uuge7oU9BRdfX6o99tiDJk2aFLVv3H///eoS\nXWQnkOQMpMDMJgOPEhrTAXD3f6RbyMz+BpwILHP3vDjut8BJwAbC/0nOdfdVcdrVwHBgM3CJu/+r\n/G9ne2VddpstgwcPZsCAAdtckTVkyBBOOukkOnXqRH5+Pvvvv/92y7Vu3ZrTTjuNvLw82rdvX9R1\n+q677sqUKVO45JJLWL16NZs2beJnP/sZHTt2LDWG4l2YJ11+9OjRDBo0iCZNmnDkkUfy8ccfA3DS\nSScxcOBAHnvsMe6+++5tlhk/fjwjR45k3bp1fPe73+W+++7LaLuJSM1RZnfuZlbSkcDdvfSbWofl\nDgfWAn9PSSDHAM+5+yYzuyWu6Eoz6wBMBHoALYH/AD9w983pylB37pILtM/t5NSde+ncvezbx5W8\n3Atm1q7YuH+nvHwVGBiH+wOT3P1b4GMzm09IJplV+IuISNYl6QurlZk9YmbL4uP/zKxVJZQ9DHgm\nDu8DfJYybWEcV1I8I8yswMwKli9fXglhiIhIJpI0ot8HPE6oWmoJPBHHZczMrgU2AeW7QTfg7mPd\nPd/d85s3b17aPBUJTyQx7WuyM0uSQJq7+33uvik+xgElH7kTMLNzCI3rQ3zrt28R0DpltlZxXLnV\nrVuXFStW6IstWefurFixgrp161Z3KCLVIslVWCvM7ExCIzfAYGBFJoWZ2bHAL4Ej3H1dyqTHgQfN\n7HbCWc73gdczKaNVq1YsXLgQVW9JVahbty6tWlVGja5IzZMkgQwD7gbuABx4GSizYd3MJgJ9gGZm\nthAYBVwN7AY8a+HKhVfdfaS7v21mDwHvEKq2flrWFVilqVOnTpn/nBYRkYor8zLeXFbSZbwiIlVK\nl/GWWMAv3f1WM7ubcOaxDXe/pKKFi4hIzZWuCquw+xL9xBcRke2UmkDc/Yk4uM7dH06dZmaDSlhE\nRER2Ikku47064TgREdmJpGsDOQ44HtjHzH6fMml3wpVSIiKyE0vXBrKY0P7RD5iZMn4N8PNsBiUi\nIrkvXRvIHGCOmT3o7hurMCYREakBkvyRsJ2Z3QR0AIr6bHD372YtKhERyXlJO1P8E6Hdoy/wd+CB\nbAYlIrINs2QPqVJJEkg9d59K+Nf6J+4+Gjghu2GJiEiuS1KF9a2Z7QJ8YGYXEXrJbZjdsEREJNcl\nOQO5FKgPXAJ0B84ChmYzKBERyX1Jbmk7Iw6uJUEvvCIisnMoM4GY2TRK7kzxyKxEJCIiNUKSNpAr\nUobrAqeif6KLiOz0klRhzSw26iUzy+hugSIisuNIUoXVNOXlLoSG9D2yFpGIiNQISaqwZhLaQIxQ\ndfUxMDybQYmISO5LUoWlG4yLiMh20nXnfkq6Bd39H+mmm9nfgBOBZe6eF8c1BSYD7YAFwGnu/qWZ\nGXAXofv4dcA57j4r+dsQEZGqlu4M5KQ00xxIm0CAccAfCH1nFboKmOruN5vZVfH1lcBxwPfjoyeh\n762eZaxbqreYAAAU8klEQVRfRESqUbru3Cv0p0F3f8HM2hUb3R/oE4fHA9MJCaQ/8Hd3d+BVM2ts\nZi3cfUlFYhARkexJ0oiOmZ0AdGTb7tyvz6C8vVKSwufAXnF4H+CzlPkWxnHbJRAzGwGMAGjTpk0G\nIYiISGUosy8sMxsDnA5cTLgSaxDQtqIFx7ON7f7hnmC5se6e7+75zZs3r2gYIiKSoSSdKR7i7mcD\nX7r7dcDBwA8yLG+pmbUAiM/L4vhFQOuU+VrFcSIikqOSJJBv4vM6M2sJbARaZFje42ztyXco8FjK\n+LMt6AWsVvuHiEhuS9IG8qSZNQZ+C8wiVDv9payFzGwiocG8mZktBEYBNwMPmdlw4BPgtDj704RL\neOcTLuNVr78iIjnOQlNEwpnNdgPquvvq7IWUXH5+vhcUFFR3GCKSbUlvV1uO41mlyeXYSmFmM909\nv6LrSdKIPtfMrjGzfd3921xJHiIiUr2StIGcROgD6yEzm2FmV5iZrp8VEdnJlZlA3P0Td7/V3bsD\nPwY6EzpUFBGRnVjSPxK2JfwX5HRgM/DLbAYlIiK5L8n9QF4D6gAPAYPc/aOsRyUiIjkvyRnI2e7+\nXtYjERGRGqXUNhAzuxPA3d8zs0uLTRuX5bhERCTHpWtEPzxleGixaZ2zEIuIiNQg6RKIlTIsIiKS\ntg1kFzNrQkgyhcOFiaRW1iMTEZGcli6B7AHMZGvSSL3FbO78J19ERKpFujsStqvCOEREpIZJ0pWJ\niIjIdpRAREQkI0ogIiKSkSTduf/OzDpWRTAiIlJzJDkDmQeMNbPXzGykme2R7aBERCT3JenO/V53\nPxQ4G2gHzDWzB82sb7aDExGR3JWoDcTMagH7x8cXwBzgMjOblMXYREQkhyVpA7kDeBc4HrjR3bu7\n+y3ufhLQNZNCzeznZva2mb1lZhPNrK6ZtY/VZPPNbLKZ7ZrJukVEpGokOQOZC3Rx9wvc/fVi03qU\nt0Az2we4BMh39zxCtyhnALcAd7j794AvgeHlXbeIVIBZsodIlCSBrCLlH+tm1tjMTgZw99UZllsb\nqGdmtYH6wBLgSGBKnD4eODnDdYuISBVIkkBGpSYKd18FjMq0QHdfBNwGfEpIHKsJfW6tcvdNcbaF\nwD4lLW9mI8yswMwKli9fnmkYIiJSQUkSSEnzJLqXeklir779gfZAS6ABcGzS5d19rLvnu3t+8+bN\nMw1DREQqKEkCKTCz281s3/i4nXDGkKkfAh+7+3J33wj8AzgUaByrtABaAYsqUIaIiGRZkgRyMbAB\nmBwf3wI/rUCZnwK9zKy+mRlwFPAOMA0YGOcZCjxWgTJERCTLyqyKcvevgasqq0B3f83MphDuL7IJ\neAMYCzwFTDKzG+K4v1ZWmSIiUvlKTSBmdqe7/8zMnqCEG0i5e79MC3X3UWzfEP8RGVwWLCIi1SPd\nGcj98fm2qghERERqlnR3JJwZn5+vunBERKSmSFeF9SYl3/vcAHf3zlmLSkREcl66KqwTqywKERGp\ncdJVYX1SOGxmewEHxZevu/uybAcmIiK5LUlvvKcBrwODgNOA18xsYPqlRERkR5ekS5JrgYMKzzrM\nrDnwH7Z2fCgiIjuhRH1hFauyWpFwORER2YElOQP5p5n9C5gYX58OPJ29kEREpCZI0pXJL8zsFOCw\nOGqsuz+S3bBERCTXpU0g8cZR3wPedPfLqiYkERGpCUptyzCze4CfA3sCvzGzX1dZVCIikvPSnYEc\nDhzo7pvNrD7wIvCbqglLRERyXbqrqTa4+2YAd19H6MJEREQESH8Gsr+ZzY3DBuwbX6svLBERSZtA\nDqiyKEREpMZJ1BeWiIhIcfpHuYiIZEQJREREMpLufyBT4/MtlV2omTU2sylm9q6ZzTOzg82sqZk9\na2YfxOcmlV2uiIhUnnRnIC3M7BCgn5l1NbNuqY8KlnsX8E933x84EJgHXAVMdffvA1PjaxERyVHp\nrsL6H+DXQCvg9mLTHDgykwLNbA/CnxTPAXD3DcAGM+sP9ImzjQemA1dmUoaIiGRfuquwpgBTzOzX\n7l6Z/0BvDywH7jOzA4GZwKXAXu6+JM7zObBXSQub2QhgBECbNm0qMSwRESmPMhvR3f03ZtbPzG6L\nj4reK7020A34k7t3Bb6mWHWVuzvhLKekeMa6e7675zdv3ryCoYiISKaS3NL2JsIZwjvxcamZ3ViB\nMhcCC939tfh6CiGhLDWzFrHMFoDuuy4iksOS3FDqBKCLu28BMLPxwBvANZkU6O6fm9lnZrafu78H\nHMXW5DQUuDk+P5bJ+kUku6ZPL7tbvD59SqxAyLoksUH1xJfLsWUqSQIBaAysjMN7VEK5FwMTzGxX\n4CPgXMLZ0ENmNhz4BDitEsoRyS1W9kFk+rRkq6pJBxrZMSVJIDcBb5jZNEJHiodTwUts3X02kF/C\npKMqsl4REak6SW5pO9HMpgMHxVFXuvvnWY1KRERyXqIqrHh57eNZjkVERGoQ9YUlIiIZUQIREZGM\npE0gZlbLzN6tqmBERKTmSJtA4j3R3zMz9RkiIiLbSNKI3gR428xeJ3Q7AoC798taVCIikvOSJJBf\nZz0KERGpcZL8D+R5M2sLfN/d/2Nm9YFa2Q9NRERyWZLOFM8ndHj45zhqH+DRbAYlIiK5L8llvD8F\nDgW+AnD3D4DvZDMoERHJfUkSyLfxroEAmFltSrlXh4iI7DySJJDnzewaoJ6ZHQ08DDyR3bBERCTX\nJUkgVxFuQfsmcAHwNPCrbAYlIiK5L8lVWFviTaReI1RdvRdvOSsiIjuxMhOImZ0AjAE+JNwPpL2Z\nXeDuz2Q7OBERyV1J/kj4O6Cvu88HMLN9gacAJRDJPQnu+Ae6659IZUjSBrKmMHlEHwFrshSPiIjU\nEKWegZjZKXGwwMyeBh4itIEMAmZUQWwiIpLD0lVhnZQyvBQ4Ig4vB+pVtGAzqwUUAIvc/UQzaw9M\nAvYEZgJnpf7/REREckupCcTdz81y2ZcC84Dd4+tbgDvcfZKZjQGGA3/KcgwiIpKhJH1htTez283s\nH2b2eOGjIoWaWSvgBODe+NqAIwl9bgGMB06uSBkiIpJdSa7CehT4K+Hf51sqqdw7gV8CjeLrPYFV\n7r4pvl5I6LRxO2Y2AhgB0KaN7nMlIlJdkiSQ9e7++8oq0MxOBJa5+0wz61Pe5d19LDAWID8/X9dY\niohUkyQJ5C4zGwX8G/i2cKS7z8qwzEOBfmZ2PFCX0AZyF9DYzGrHs5BWwKIM1y8iIlUgSQLpBJxF\naKMorMLy+Lrc3P1q4GqAeAZyhbsPMbOHgYGEK7GGAo9lsn4REakaSRLIIOC7VXBJ7ZXAJDO7AXiD\n0O4iuUj/9hYRkiWQt4DGwLLKLtzdpwPT4/BHQI/KLkNERLIjSQJpDLxrZjPYtg2kX9aiEhGRnJck\ngYzKehQiIlLjJLkfyPNVEYiIiNQsSe4Hsoat90DfFagDfO3uu5e+lIiI7OiSnIEU/lu8sMuR/kCv\nbAYlIiK5L0kbSJF4K9tH4x8Lr8pOSAJU6qWyukxWRLIhSRXWKSkvdwHygfVZi0hERGqEJGcgqfcF\n2QQsIFRjiYjITixJG0i27wsiIiI1ULpb2v5PmuXc3X+ThXhERKSGSHcG8nUJ4xoQ7hS4J6AEIiKy\nE0t3S9vfFQ6bWSPCLWjPJfSW+7vSlhMRkZ1D2jYQM2sKXAYMIdxmtpu7f1kVgYmISG5L1wbyW+AU\nwt3/Orn72iqLSkREct4uaaZdDrQEfgUsNrOv4mONmX1VNeGJiEiuStcGki65iIjITk5JQkREMqIE\nIiIiGVECERGRjFR5AjGz1mY2zczeMbO3zezSOL6pmT1rZh/E5yZVHZuIiCRXHWcgm4DL3b0D4b4i\nPzWzDoTu4ae6+/eBqai7eBGRnFblCcTdl7j7rDi8BpgH7EPo4Xd8nG08cHJVxyYiIsmV64ZSlc3M\n2gFdgdeAvdx9SZz0ObBXKcuMAEYAtGnTpiKFJ5otyQ2bQDdtEpGdT7U1optZQ+D/gJ+5+zZ/TIx3\nPizxiOzuY909393zmzdvXgWRiohISaolgZhZHULymODu/4ijl5pZizi9BbCsOmITEZFkquMqLAP+\nCsxz99tTJj0ODI3DQ4HHqjo2ERFJrjraQA4FzgLeNLPZcdw1wM3AQ2Y2HPgEOK0aYhMRkYSqPIG4\n+3+B0lqwj6rKWEREJHP6J7qIiGRECURERDKiBCIiIhlRAhERkYwogYiISEaUQEREJCNKICIikhEl\nEBERyYgSiIiIZEQJREREMqIEIiIiGVECERGRjCiBiIhIRpRAREQkI0ogIiKSESUQERHJSHXckXCH\nZNeVdo+sbfkoz3Ik28vl2CBZfLkcG+hzLS6XY4Pcji+XYytOZyAiIpIRJRAREclIziUQMzvWzN4z\ns/lmdlV1xyMiIiXLqQRiZrWAPwLHAR2AwWbWoXqjEhGRkuRUAgF6APPd/SN33wBMAvpXc0wiIlIC\nc6/+lvxCZjYQONbdz4uvzwJ6uvtFKfOMAEbEl/sB71V5oNAM+KIayk0il2OD3I5PsWUul+NTbNtr\n6+7NK7qSGncZr7uPBcZWZwxmVuDu+dUZQ2lyOTbI7fgUW+ZyOT7Flj25VoW1CGid8rpVHCciIjkm\n1xLIDOD7ZtbezHYFzgAer+aYRESkBDlVheXum8zsIuBfQC3gb+7+djWHVZJqrUIrQy7HBrkdn2LL\nXC7Hp9iyJKca0UVEpObItSosERGpIZRAREQkI0ogaZTVrYqZnWNmy81sdnycV83x3JESy/tmtipl\n2uaUaZV+YUKC2NqY2TQze8PM5prZ8SnTro7LvWdmP6rs2CoSn5m1M7NvUrbdmGqIra2ZTY1xTTez\nVinThprZB/ExNMdiq+59Ll1st5rZ22Y2z8x+b2bJusDNLM6/mdkyM3urlOkWY5gfY+2WrVgqnbvr\nUcKD0Ij/IfBdYFdgDtCh2DznAH/IlXiKzX8x4SKEwtdrq3lbjQUujMMdgAUpw3OA3YD2cT21cii+\ndsBb1bztHgaGxuEjgfvjcFPgo/jcJA43yYXYcmSfK227HQK8FNdRC3gF6JPFWA8HupW2HwHHA88A\nBvQCXstWLJX90BlI6XKtW5XyxjMYmFglkSWLzYHd4/AewOI43B+Y5O7fuvvHwPy4vlyJL9uSxNYB\neC4OT0uZ/iPgWXdf6e5fAs8Cx+ZIbNlWkdgcqEtIPLsBdYCl2QrU3V8AVqaZpT/wdw9eBRqbWYts\nxVOZlEBKtw/wWcrrhXFccafG084pZta6hOlVHQ9m1pbwa/65lNF1zazAzF41s5OrIbbRwJlmthB4\nmnCGlHTZ6owPoH2s2nrezHpXQ2xzgFPi8ACgkZntmXDZ6ooNqn+fKzE2d3+FkFCWxMe/3H1eJcdX\nHlXxHcgKJZCKeQJo5+6dCb/+xldzPIXOAKa4++aUcW09dJnwY+BOM9u3imMaDIxz91aEU/b7zSyX\n9r/S4lsCtHH3rsBlwINmtnua9WTDFcARZvYGcAShd4bN6RepMuliq+59rsTYzOx7wAGEni72AY7M\nwg+DnUIufYFzTZndqrj7Cnf/Nr68F+henfGkOINi1Vfuvig+fwRMB7pWcWzDgYdiDK8QqhCaJVy2\n2uKLVWsr4viZhHr3H1RlbO6+2N1PiUns2jhuVZJlqzG2at/n0sQ2AHjV3de6+1pC+8PBlRhbedXc\nLpyquxEmVx+Ef+l/RKgKKmyk61hsnhYpw4U7ZbXFE+fbH1hA/JNoHNcE2C0ONwM+IE0DfJa21TPA\nOXH4AEIbgwEd2bYR/SMqvxG9IvE1L4yH0GC7CGhaxbE1A3aJw/8LXB+HmwIfx8+3SRzOldhyYZ8r\nLbbTgf/EddQBpgInVeY+V0K87Si9Ef0Etm1Efz2bsVTq+6ruAHL5QajKeJ/wq/PaOO56oF8cvgl4\nO+6804D9qzOe+Ho0cHOx5Q4B3oxxvgkMr4Zt1YFw5cscYDZwTMqy18bl3gOOq6bPssT4gFPjZzwb\nmJWNA02C2AbGA/D7hDPd3VKWHUa48GA+cG6uxJYj+1xpsdUC/gzMA94Bbs/GPpcS50RCVehGQvvG\ncGAkMDJON8KN9D6M2yo/m/FU5kNdmYiISEbUBiIiIhlRAhERkYwogYiISEaUQEREJCNKICIikhEl\nEKnRUnp8fdvM5pjZ5YX/cDezfDP7fTXHd02aaQvM7M2UHmsPyWD9J5tZh4pFKZIZXcYrNZqZrXX3\nhnH4O8CDwEvuPqp6IwtS4yth2gLCNf9fVGD944An3X1KOZap7e6bMi1TpJDOQGSH4e7LgBHARfEe\nC33M7EkAMzsi5Zf+G2bWKI6/Mp4FzDGzm+O4LrEDwLlm9oiZNYnjp5tZfhxuFhNA4X1h/mFm/4z3\n5bg1jr8ZqBfLnJDkPZhZw3gPi1kxrv4p086OMc0xs/vjGUs/4LexjH3LiP1OMysALq2EzS2if6Lr\nUbMflHDPCWAVsBfQh/DrHELHl4fG4YaEbiyOA14G6sfxTePzXOCIOHw9cGccnk78lzChm4wFcfgc\nQrcaexD60PoEaF1afClxLiD883g28R4QMa7dU8qYz9YuX94n9M+VGus4YGDKOtPFfk91f1567FgP\nnYHIzuIl4HYzuwRo7KEK54fAfe6+DsDdV5rZHnH683G58YQbApVlqruvdvf1hO4x2iaMq6+7d3H3\nnvG1ATea2VxCf037EJLhkcDDHqu73H27+0skiH1ywphEElECkR2KmX2X0J34stTx7n4zcB5QD3jJ\nzPbPYPWb2PqdqVts2rcpw5sJZxKZGELowLG7u3ch3OioeFmZ+rqS1iMCKIHIDsTMmgNjCLcZ9mLT\n9nX3N939FmAGodfiZ4Fzzax+nKepu68Gvky5P8RZQOEv+gVs7bJ/YMKwNppZnXK8jT2AZe6+0cz6\nsvVM5jlgUOHNmsysaRy/BmgEUEbsIpUu019JIrminpnNJnTLvQm4H7i9hPl+Fg/IWwi96z7j7t+a\nWRegwMw2EO5EeA0wFBgTE8tHwLlxHbcBD5nZCOCphPGNBeaa2Sx3H5Jg/gnAE2b2JlAAvAvg7m+b\n2f8Cz5vZZuANQtvLJOAvsWpuYJrYRSqdLuMVEZGMqApLREQyogQiIiIZUQIREZGMKIGIiEhGlEBE\nRCQjSiAiIpIRJRAREcnI/wOaXUJ+f34acAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd681253d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot it instead!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ind = np.arange(num_tests)  # the x locations for the groups\n",
    "width = 0.25       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, tally_PE[:,0], width, color='r')\n",
    "rects2 = ax.bar(ind + width, tally_PE[:,1], width, color='y')\n",
    "rects3 = ax.bar(ind + 2*width, tally_PE[:,2], width, color='g')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Number of Policy Evaluations')\n",
    "ax.set_xlabel('Discount Factor')\n",
    "ax.set_title('Number of Policy Evaluations for Different Discount Factors')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('0.5', '0.75', '0.80', '0.85', '0.90', '0.95', '0.98', '1.0'))\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('Policy Iteration', 'Modified Policy Iteration', 'Value Iteration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. Value iteration in this case converges in 4 iterations. The modified policy evaluation plateaus again for discount factors greater than 0.95. Now let's see if the policies are the same for the policy iteration case of low and high discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50 Discount Factor\n",
      "Policy Probability Distribution:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[[ 0.   -1.   -1.5  -1.75]\n",
      " [-1.   -1.5  -1.75 -1.5 ]\n",
      " [-1.5  -1.75 -1.5  -1.  ]\n",
      " [-1.75 -1.5  -1.    0.  ]]\n",
      "\n",
      "1.00 Discount Factor\n",
      "Policy Probability Distribution:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "[[ 0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  2.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 0.  2.  0.  0.]\n",
      " [ 0.  2.  0.  0.]\n",
      " [ 0.  2.  0.  0.]\n",
      " [ 0.  0.  0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration comparison\n",
    "# 0.50 Discount Factor\n",
    "policy_50, v = policy_improvement(env, discount_factor=0.50)\n",
    "print(\"0.50 Discount Factor\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "# 1.00 Discount Factor\n",
    "policy_100, v = policy_improvement(env, discount_factor=1.)\n",
    "print(\"1.00 Discount Factor\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "# Are both policies the same? It's hard to compare by eye\n",
    "# if there are ones in the matrix, then no, if 0s and 2s, then yes\n",
    "print policy_50+policy_100    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both policies are the same. The values are not because of the short and far-sightedness differences of both agents.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
